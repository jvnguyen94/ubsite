{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-155-4c6356020c63>\", line 11, in <module>\n",
      "    train_dataset = ProteinDataset(ids=train_files)\n",
      "TypeError: __init__() missing 1 required positional argument: 'root'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"_summary_\n",
    "## masking some of the nodes in the training sites\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_files = file_names[0:2]\n",
    "test_files = file_names[2:4]\n",
    "\n",
    "train_dataset = ProteinDataset(ids=train_files)\n",
    "# test_dataset = ProteinDataset(ids=test_files)\n",
    "\n",
    "# train_dataset.process()\n",
    "# test_dataset.process()\n",
    "#data = data_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids, transform=None, pre_transform=None):\n",
    "        #super().__init__()\n",
    "        ##file names equiv to ids\n",
    "        super().__init__()\n",
    "        # self.root = root\n",
    "        self.raw_file_names = list(ids)\n",
    "        self.processed_file_names = []\n",
    "    # @property\n",
    "    # def raw_file_names(self):\n",
    "\n",
    "    #     return [filename for filename in os.scandir(self.root)]\n",
    "\n",
    "    # @property\n",
    "    # def processed_file_names(self):\n",
    "    #     return [os.path.splitext(os.path.basename(file))[0]+'.pt' for file in self.raw_paths]\n",
    "\n",
    "    def len(self):\n",
    "        ## Can iter through len and load through batches\n",
    "        return len(self.raw_file_names)\n",
    "\n",
    "    \n",
    "    def process(self):\n",
    "\n",
    "        \n",
    "        data_list = []\n",
    "        count = 0\n",
    "                \n",
    "        for file in tqdm(self.raw_file_names):\n",
    "        #    if (pathlib.Path(file).suffix == \".pdb\"):\n",
    "            struct = self._get_structure(file)\n",
    "            # print (struct)\n",
    "            seq = self._get_sequence(struct)\n",
    "            #print(seq)\n",
    "            #print(self._get_SeqVecEmbedder(seq))\n",
    "            # node features extracted\n",
    "            node_feats = self._get_one_hot_symbftrs(seq)\n",
    "            # print(node_feats)\n",
    "            # edge-index extracted\n",
    "            mat = self._get_adjacency(file)\n",
    "            #print(mat)\n",
    "            \n",
    "            edge_index = self._get_edgeindex(file, mat)\n",
    "            #print(f'Node features size :{torch.Tensor.size(node_feats)}')\n",
    "            #print(f'Matrix size :{mat.shape}')\n",
    "            # create data object\n",
    "            ### ADD Y = TARGET UB SITE AS binary fts (0./1)\n",
    "            pos = ub_list.loc[ub_list['protein'] == self._get_id(file), 'site']\n",
    "            #print(pos)\n",
    "            ub_label = self._get_ubsite(seq, pos)\n",
    "            #print(ub_label)\n",
    "            data = Data(x=torch.tensor(node_feats, dtype=torch.float32),\n",
    "                        y=torch.tensor(ub_label, dtype=torch.float32), \n",
    "                        edge_index=torch.tensor(edge_index, dtype=torch.long))\n",
    "            #print(data)\n",
    "            count += 1\n",
    "            print(count)\n",
    "            #print(\"asdf\")\n",
    "            data_list.append(data)\n",
    "            self.processed_file_names.append(\n",
    "                \"./d/processed/\" + f'data_{count}.pt')\n",
    "            torch.save(data, \"./d/processed/\" + f'data_{count}.pt')\n",
    "            \n",
    "        return data_list\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        return torch.load(f'./d/processed/data_{idx}.pt')\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _get_id(self, pdb_file):\n",
    "        pdb_id = pdb_file.split(\".\")[-2].split('/')[-1]\n",
    "        #print(pdb_id)\n",
    "        return pdb_id\n",
    "    \n",
    "    # get structure from a pdb file\n",
    "    # Uses biopython\n",
    "    def _get_structure(self, file):\n",
    "        parser = PDBParser()\n",
    "        structure = parser.get_structure(self._get_id(file), file)\n",
    "        return structure\n",
    "\n",
    "    # Function to get sequence from pdb structure\n",
    "    # Uses structure made using biopython\n",
    "    # Those residues for which symbols are U / X are converted into A\n",
    "\n",
    "    \n",
    "    def _get_sequence(self, structure):\n",
    "        sequence = \"\"\n",
    "        for model in structure:\n",
    "          for chain in model:\n",
    "            for residue in chain:\n",
    "              if residue.get_resname() in ressymbl.keys():\n",
    "                  sequence = sequence + ressymbl[residue.get_resname()]\n",
    "        return sequence\n",
    "        # One hot encoding for symbols\n",
    "\n",
    "    def _get_ubsite(self,sequence, position):\n",
    "        n = len(sequence)\n",
    "        ub_array = np.zeros((n,))\n",
    "        ub_array[position] = 1\n",
    "        \n",
    "        return ub_array\n",
    "\n",
    "    def _get_one_hot_symbftrs(self, sequence):\n",
    "        one_hot_symb = np.zeros((len(sequence), len(pro_res_table)))\n",
    "        row = 0\n",
    "        for res in sequence:\n",
    "          col = pro_res_table.index(res)\n",
    "          one_hot_symb[row][col] = 1\n",
    "          row += 1\n",
    "        return torch.tensor(one_hot_symb, dtype=torch.float)\n",
    "    \n",
    "    def _get_adjacency(self, file):\n",
    "        \n",
    "        ## Check how I can look into side chains vs AlphaChains in terms of close proximity (is it avg distance or min distance between two residues?)\n",
    "        edge_ind =[]\n",
    "        molecule = bg.Pmolecule(file)\n",
    "        network = molecule.network()\n",
    "        mat = nx.adjacency_matrix(network)\n",
    "        m = mat.todense()\n",
    "        return m\n",
    "    # get adjacency matrix in coo format to pass in GCNN model\n",
    "\n",
    "    def _get_edgeindex(self, file, adjacency_mat):\n",
    "        edge_ind = []\n",
    "        m = self._get_adjacency(file)\n",
    "        # check_symmetric(m, rtol=1e-05, atol=1e-08)\n",
    "\n",
    "        a = np.nonzero(m > 0)[0]\n",
    "        b = np.nonzero(m > 0)[1]\n",
    "        edge_ind.append(a)\n",
    "        edge_ind.append(b)\n",
    "        return torch.tensor(np.array(edge_ind), dtype=torch.long)\n",
    "       # Residue features calculated from pcp_dict\n",
    "\n",
    "    def _get_res_ftrs(self, sequence):\n",
    "        res_ftrs_out = []\n",
    "        for res in sequence:\n",
    "          res_ftrs_out.append(pcp_dict[res])\n",
    "        res_ftrs_out = np.array(res_ftrs_out)\n",
    "        # print(res_ftrs_out.shape)\n",
    "        return torch.tensor(res_ftrs_out, dtype=torch.float)\n",
    "    \n",
    "    # total features after concatenating one_hot_symbftrs and res_ftrs\n",
    "\n",
    "    def _get_node_ftrs(self, sequence):\n",
    "        one_hot_symb = one_hot_symbftrs(sequence)\n",
    "        res_ftrs_out = res_ftrs(sequence)\n",
    "        return torch.tensor(np.hstack((one_hot_symb, res_ftrs_out)), dtype=torch.float)\n",
    "\n",
    "    def _get_SeqVecEmbedder(self, seq):\n",
    "\n",
    "        embedder = SeqVecEmbedder()\n",
    "        embedding = embedder.embed(seq)\n",
    "        protein_embd = torch.tensor(embedding).sum(dim=0)  # Vector with shape [L x 1024]\n",
    "        np_arr = protein_embd.cpu().detach().numpy()\n",
    "        \n",
    "        return np_arr\n",
    "    \n",
    "    def _get_ProtBertEmbedder(self, seq):\n",
    "\n",
    "        embedder = ProtTransBertBFDEmbedder()\n",
    "        embedding = embedder.embed(seq)\n",
    "        protein_embd = torch.tensor(embedding).sum(dim=0)  # Vector with shape [L x 1024]\n",
    "        np_arr = protein_embd.cpu().detach().numpy()\n",
    "        \n",
    "        return np_arr\n",
    "\n",
    "# #%%    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-157-4c6356020c63>\", line 11, in <module>\n",
      "    train_dataset = ProteinDataset(ids=train_files)\n",
      "  File \"<ipython-input-156-d53060bb82d1>\", line 8, in __init__\n",
      "    super().__init__()\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/data/dataset.py\", line 115, in __init__\n",
      "    self._process()\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/data/dataset.py\", line 253, in _process\n",
      "    if not self.force_reload and files_exist(self.processed_paths):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/data/dataset.py\", line 212, in processed_paths\n",
      "    files = self.processed_file_names\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/data/dataset.py\", line 71, in processed_file_names\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"_summary_\n",
    "## masking some of the nodes in the training sites\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_files = file_names[0:2]\n",
    "test_files = file_names[2:4]\n",
    "\n",
    "train_dataset = ProteinDataset(ids=train_files)\n",
    "# test_dataset = ProteinDataset(ids=test_files)\n",
    "\n",
    "# train_dataset.process()\n",
    "# test_dataset.process()\n",
    "#data = data_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpre_transform\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpre_filter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mforce_reload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Dataset base class for creating graph datasets.\n",
      "See `here <https://pytorch-geometric.readthedocs.io/en/latest/tutorial/\n",
      "create_dataset.html>`__ for the accompanying tutorial.\n",
      "\n",
      "Args:\n",
      "    root (str, optional): Root directory where the dataset should be saved.\n",
      "        (optional: :obj:`None`)\n",
      "    transform (callable, optional): A function/transform that takes in a\n",
      "        :class:`~torch_geometric.data.Data` or\n",
      "        :class:`~torch_geometric.data.HeteroData` object and returns a\n",
      "        transformed version.\n",
      "        The data object will be transformed before every access.\n",
      "        (default: :obj:`None`)\n",
      "    pre_transform (callable, optional): A function/transform that takes in\n",
      "        a :class:`~torch_geometric.data.Data` or\n",
      "        :class:`~torch_geometric.data.HeteroData` object and returns a\n",
      "        transformed version.\n",
      "        The data object will be transformed before being saved to disk.\n",
      "        (default: :obj:`None`)\n",
      "    pre_filter (callable, optional): A function that takes in a\n",
      "        :class:`~torch_geometric.data.Data` or\n",
      "        :class:`~torch_geometric.data.HeteroData` object and returns a\n",
      "        boolean value, indicating whether the data object should be\n",
      "        included in the final dataset. (default: :obj:`None`)\n",
      "    log (bool, optional): Whether to print any console output while\n",
      "        downloading and processing the dataset. (default: :obj:`True`)\n",
      "    force_reload (bool, optional): Whether to re-process the dataset.\n",
      "        (default: :obj:`False`)\n",
      "\u001b[0;31mFile:\u001b[0m           ~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/data/dataset.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     InMemoryDataset, OnDiskDataset, ProteinDataset, ProteinDataset, ProteinDataset, ProteinDataset, ProteinDataset, ProteinDataset, ProteinDataset, ProteinDataset, ..."
     ]
    }
   ],
   "source": [
    "?Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:65\u001b[0;36m\u001b[0m\n\u001b[0;31m    return data_list\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids, transform=None, pre_transform=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.ids = ids\n",
    "        \n",
    "        \n",
    "    def len(self):\n",
    "        ## Can iter through len and load through batches\n",
    "        return len(self.ids)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        return torch.load(f'./d/processed/data_{idx}.pt')\n",
    "    \n",
    "\n",
    "  \n",
    "# #%%    \n",
    "\n",
    "class PreProcessor():\n",
    "    # init etc\n",
    "    def process(self):\n",
    "\n",
    "        data_list = []\n",
    "        count = 0\n",
    "\n",
    "        for file in tqdm(self.raw_file_names):\n",
    "                #    if (pathlib.Path(file).suffix == \".pdb\"):\n",
    "                struct = self._get_structure(file)\n",
    "                # print (struct)\n",
    "                seq = self._get_sequence(struct)\n",
    "                # print(seq)\n",
    "                # print(self._get_SeqVecEmbedder(seq))\n",
    "                # node features extracted\n",
    "                node_feats = self._get_one_hot_symbftrs(seq)\n",
    "                # print(node_feats)\n",
    "                # edge-index extracted\n",
    "                mat = self._get_adjacency(file)\n",
    "                # print(mat)\n",
    "\n",
    "                edge_index = self._get_edgeindex(file, mat)\n",
    "                # print(f'Node features size :{torch.Tensor.size(node_feats)}')\n",
    "                # print(f'Matrix size :{mat.shape}')\n",
    "                # create data object\n",
    "                # ADD Y = TARGET UB SITE AS binary fts (0./1)\n",
    "                pos = ub_list.loc[ub_list['protein'] == self._get_id(file), 'site']\n",
    "                # print(pos)\n",
    "                ub_label = self._get_ubsite(seq, pos)\n",
    "                # print(ub_label)\n",
    "                data = Data(x=torch.tensor(node_feats, dtype=torch.float32),\n",
    "                            y=torch.tensor(ub_label, dtype=torch.float32),\n",
    "                            edge_index=torch.tensor(edge_index, dtype=torch.long))\n",
    "                # print(data)\n",
    "                count += 1\n",
    "                print(count)\n",
    "                # print(\"asdf\")\n",
    "                data_list.append(data)\n",
    "                self.processed_file_names.append(\n",
    "                    \"./d/processed/\" + f'data_{count}.pt')\n",
    "                torch.save(data, \"./d/processed/\" + f'data_{count}.pt')\n",
    "\n",
    "            return data_list\n",
    "\n",
    "    # get structure from a pdb file\n",
    "        # Uses biopython\n",
    "        def _get_structure(self, file):\n",
    "            parser = PDBParser()\n",
    "            structure = parser.get_structure(self._get_id(file), file)\n",
    "            return structure\n",
    "\n",
    "        # Function to get sequence from pdb structure\n",
    "        # Uses structure made using biopython\n",
    "        # Those residues for which symbols are U / X are converted into A\n",
    "\n",
    "        \n",
    "        def _get_sequence(self, structure):\n",
    "            sequence = \"\"\n",
    "            for model in structure:\n",
    "            for chain in model:\n",
    "                for residue in chain:\n",
    "                if residue.get_resname() in ressymbl.keys():\n",
    "                    sequence = sequence + ressymbl[residue.get_resname()]\n",
    "            return sequence\n",
    "            # One hot encoding for symbols\n",
    "\n",
    "        def _get_ubsite(self,sequence, position):\n",
    "            n = len(sequence)\n",
    "            ub_array = np.zeros((n,))\n",
    "            ub_array[position] = 1\n",
    "            \n",
    "            return ub_array\n",
    "\n",
    "        def _get_one_hot_symbftrs(self, sequence):\n",
    "            one_hot_symb = np.zeros((len(sequence), len(pro_res_table)))\n",
    "            row = 0\n",
    "            for res in sequence:\n",
    "            col = pro_res_table.index(res)\n",
    "            one_hot_symb[row][col] = 1\n",
    "            row += 1\n",
    "            return torch.tensor(one_hot_symb, dtype=torch.float)\n",
    "        \n",
    "        def _get_adjacency(self, file):\n",
    "            \n",
    "            ## Check how I can look into side chains vs AlphaChains in terms of close proximity (is it avg distance or min distance between two residues?)\n",
    "            edge_ind =[]\n",
    "            molecule = bg.Pmolecule(file)\n",
    "            network = molecule.network()\n",
    "            mat = nx.adjacency_matrix(network)\n",
    "            m = mat.todense()\n",
    "            return m\n",
    "        # get adjacency matrix in coo format to pass in GCNN model\n",
    "\n",
    "        def _get_edgeindex(self, file, adjacency_mat):\n",
    "            edge_ind = []\n",
    "            m = self._get_adjacency(file)\n",
    "            # check_symmetric(m, rtol=1e-05, atol=1e-08)\n",
    "\n",
    "            a = np.nonzero(m > 0)[0]\n",
    "            b = np.nonzero(m > 0)[1]\n",
    "            edge_ind.append(a)\n",
    "            edge_ind.append(b)\n",
    "            return torch.tensor(np.array(edge_ind), dtype=torch.long)\n",
    "        # Residue features calculated from pcp_dict\n",
    "\n",
    "        def _get_res_ftrs(self, sequence):\n",
    "            res_ftrs_out = []\n",
    "            for res in sequence:\n",
    "            res_ftrs_out.append(pcp_dict[res])\n",
    "            res_ftrs_out = np.array(res_ftrs_out)\n",
    "            # print(res_ftrs_out.shape)\n",
    "            return torch.tensor(res_ftrs_out, dtype=torch.float)\n",
    "        def _get_id(self, pdb_file):\n",
    "            pdb_id = pdb_file.split(\".\")[-2].split('/')[-1]\n",
    "            #print(pdb_id)\n",
    "            return pdb_id\n",
    "    \n",
    "        # total features after concatenating one_hot_symbftrs and res_ftrs\n",
    "\n",
    "        def _get_node_ftrs(self, sequence):\n",
    "            one_hot_symb = one_hot_symbftrs(sequence)\n",
    "            res_ftrs_out = res_ftrs(sequence)\n",
    "            return torch.tensor(np.hstack((one_hot_symb, res_ftrs_out)), dtype=torch.float)\n",
    "    \n",
    "    def _get_id(self, pdb_file):\n",
    "        pdb_id = pdb_file.split(\".\")[-2].split('/')[-1]\n",
    "        #print(pdb_id)\n",
    "        return pdb_id\n",
    "    \n",
    "        def _get_SeqVecEmbedder(self, seq):\n",
    "\n",
    "            embedder = SeqVecEmbedder()\n",
    "            embedding = embedder.embed(seq)\n",
    "            protein_embd = torch.tensor(embedding).sum(dim=0)  # Vector with shape [L x 1024]\n",
    "            np_arr = protein_embd.cpu().detach().numpy()\n",
    "            \n",
    "            return np_arr\n",
    "        \n",
    "        def _get_ProtBertEmbedder(self, seq):\n",
    "\n",
    "            embedder = ProtTransBertBFDEmbedder()\n",
    "            embedding = embedder.embed(seq)\n",
    "            protein_embd = torch.tensor(embedding).sum(dim=0)  # Vector with shape [L x 1024]\n",
    "            np_arr = protein_embd.cpu().detach().numpy()\n",
    "            \n",
    "            return np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids, transform=None, pre_transform=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.ids = ids\n",
    "        \n",
    "        \n",
    "    def len(self):\n",
    "        ## Can iter through len and load through batches\n",
    "        return len(self.ids)\n",
    "\n",
    "    def get(self, idx):\n",
    "        \n",
    "        return torch.load(f'./d/processed/data_{idx}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "## masking some of the nodes in the training sites\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_files = file_names[0:2]\n",
    "test_files = file_names[2:4]\n",
    "\n",
    "train_dataset = ProteinDataset(ids=train_files)\n",
    "# test_dataset = ProteinDataset(ids=test_files)\n",
    "\n",
    "# train_dataset.process()\n",
    "# test_dataset.process()\n",
    "#data = data_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProteinDataset(2)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-164-36ebfda06ed7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[164], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [12:12 PM] Nathaniel Evans\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[12:12 PM] Nathaniel Evans\n",
    "train_dataset.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-165-e6355eaec6f9>\", line 1, in <module>\n",
      "    train_dataset.get(0)\n",
      "  File \"<ipython-input-161-d3afe8bae45f>\", line 17, in get\n",
      "    return torch.load(f'./d/processed/data_{idx}.pt')\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/serialization.py\", line 594, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/serialization.py\", line 230, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/serialization.py\", line 211, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './d/processed/data_0.pt'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[92, 20], edge_index=[2, 856], y=[92])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-167-a58cdd9dc808>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[167], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    test_files = #file_names[2:4]\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"_summary_\n",
    "## masking some of the nodes in the training sites\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_files = ['./d/processed/data_1.pt', './d/processed/data_2.pt'] #file_names[0:2]\n",
    "test_files = #file_names[2:4]\n",
    "\n",
    "train_dataset = ProteinDataset(ids=train_files)\n",
    "# test_dataset = ProteinDataset(ids=test_files)\n",
    "\n",
    "# train_dataset.process()\n",
    "# test_dataset.process()\n",
    "#data = data_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "## masking some of the nodes in the training sites\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_files = ['./d/processed/data_1.pt', './d/processed/data_2.pt'] #file_names[0:2]\n",
    "#test_files = #file_names[2:4]\n",
    "\n",
    "train_dataset = ProteinDataset(ids=train_files)\n",
    "# test_dataset = ProteinDataset(ids=test_files)\n",
    "\n",
    "# train_dataset.process()\n",
    "# test_dataset.process()\n",
    "#data = data_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "## masking some of the nodes in the training sites\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_files = ['./d/processed/data_1.pt', './d/processed/data_2.pt'] #file_names[0:2]\n",
    "#test_files = #file_names[2:4]\n",
    "\n",
    "train_dataset = ProteinDataset(ids=train_files)\n",
    "# test_dataset = ProteinDataset(ids=test_files)\n",
    "\n",
    "# train_dataset.process()\n",
    "# test_dataset.process()\n",
    "#data = data_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids):\n",
    "\n",
    "        super().__init__()\n",
    "        self.ids = ids\n",
    "        \n",
    "    def len(self):\n",
    "        ## Can iter through len and load through batches\n",
    "        return len(self.ids)\n",
    "\n",
    "    def get(self, idx):\n",
    "        fname = self.ids[idx]\n",
    "        return torch.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "## masking some of the nodes in the training sites\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_files = ['./d/processed/data_1.pt', './d/processed/data_2.pt'] #file_names[0:2]\n",
    "#test_files = #file_names[2:4]\n",
    "\n",
    "train_dataset = ProteinDataset(ids=train_files)\n",
    "# test_dataset = ProteinDataset(ids=test_files)\n",
    "\n",
    "# train_dataset.process()\n",
    "# test_dataset.process()\n",
    "#data = data_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[92, 20], edge_index=[2, 856], y=[92])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset.get(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[92, 20], edge_index=[2, 856], y=[92])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_dataset.get(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-174-7037c0f3111b>\", line 1, in <module>\n",
      "    train_dataset.get(2)\n",
      "  File \"<ipython-input-170-aa6ea365e0ed>\", line 15, in get\n",
      "    fname = self.ids[idx]\n",
      "IndexError: list index out of range\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset.get(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset as Dataset_n\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# print(math.floor(0.8 * size))\n",
    "# Make iterables using dataloader class\n",
    "#trainset, validset = torch.utils.data.random_split(\n",
    "#    data_class, [2, 2])\n",
    "# print(trainset[0])\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size = 2,  num_workers=0)\n",
    "#testloader = DataLoader(dataset=test_dataset, batch_size = 2, num_workers=0)\n",
    "# print(\"Length\")\n",
    "# print(len(trainloader))\n",
    "# print(len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[184, 20], edge_index=[2, 1712], y=[184], batch=[184], ptr=[3])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-177-2bad1b57ce05>\", line 28, in <module>\n",
      "    loss = train()\n",
      "  File \"<ipython-input-177-2bad1b57ce05>\", line 4, in train\n",
      "    model.train()\n",
      "NameError: name 'model' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(trainloader.x, trainloader.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_correct = pred[validloader.y]\n",
    "    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-178-0401f5adc4c6>\", line 19, in <module>\n",
      "    model = GCN(hidden_channels=16)\n",
      "  File \"<ipython-input-178-0401f5adc4c6>\", line 8, in __init__\n",
      "    self.conv1 = GCNConv(trainloader.x, hidden_channels)\n",
      "AttributeError: 'DataLoader' object has no attribute 'x'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(trainloader.x, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, trainloader.y)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-179-496e547a32c0>\", line 19, in <module>\n",
      "    model = GCN(hidden_channels=16)\n",
      "  File \"<ipython-input-179-496e547a32c0>\", line 8, in __init__\n",
      "    self.conv1 = GCNConv(trainloader.x, hidden_channels)\n",
      "AttributeError: 'DataLoader' object has no attribute 'x'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(trainloader.x, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, trainloader.y)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-180-496e547a32c0>\", line 19, in <module>\n",
      "    model = GCN(hidden_channels=16)\n",
      "  File \"<ipython-input-180-496e547a32c0>\", line 8, in __init__\n",
      "    self.conv1 = GCNConv(trainloader.x, hidden_channels)\n",
      "AttributeError: 'DataLoader' object has no attribute 'x'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(trainloader.x, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, trainloader.y)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-181-fb44d064f06a>\", line 28, in <module>\n",
      "    loss = train()\n",
      "  File \"<ipython-input-181-fb44d064f06a>\", line 4, in train\n",
      "    model.train()\n",
      "NameError: name 'model' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(trainloader.x, trainloader.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_correct = pred[validloader.y]\n",
    "    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-182-cfda115620e4>\", line 28, in <module>\n",
      "    loss = train()\n",
      "  File \"<ipython-input-182-cfda115620e4>\", line 4, in train\n",
      "    model.train()\n",
      "NameError: name 'model' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-183-496e547a32c0>\", line 19, in <module>\n",
      "    model = GCN(hidden_channels=16)\n",
      "  File \"<ipython-input-183-496e547a32c0>\", line 8, in __init__\n",
      "    self.conv1 = GCNConv(trainloader.x, hidden_channels)\n",
      "AttributeError: 'DataLoader' object has no attribute 'x'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(trainloader.x, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, trainloader.y)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-184-b50ee07e6fe4>\", line 19, in <module>\n",
      "    model = GCN(hidden_channels=16)\n",
      "TypeError: __init__() missing 1 required positional argument: 'in_channels'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(in_channels = 20, hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-186-cfda115620e4>\", line 28, in <module>\n",
      "    loss = train()\n",
      "  File \"<ipython-input-186-cfda115620e4>\", line 8, in train\n",
      "    out = model(data.x, data.edge_index)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<ipython-input-185-4d17933d627d>\", line 12, in forward\n",
      "    x = self.conv1(x, edge_index)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 241, in forward\n",
      "    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 108, in gcn_norm\n",
      "    deg = scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/utils/_scatter.py\", line 168, in scatter\n",
      "    raise ImportError(\"'scatter' requires the 'torch-scatter' package\")\n",
      "ImportError: 'scatter' requires the 'torch-scatter' package\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-188-cfda115620e4>\", line 28, in <module>\n",
      "    loss = train()\n",
      "  File \"<ipython-input-188-cfda115620e4>\", line 8, in train\n",
      "    out = model(data.x, data.edge_index)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    def to(self: T, tensor: Tensor, non_blocking: bool = ...) -> T:\n",
      "  File \"<ipython-input-185-4d17933d627d>\", line 12, in forward\n",
      "    x = self.conv1(x, edge_index)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    def to(self: T, tensor: Tensor, non_blocking: bool = ...) -> T:\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 241, in forward\n",
      "    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 108, in gcn_norm\n",
      "    deg = scatter(edge_weight, idx, dim=0, dim_size=num_nodes, reduce='sum')\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/utils/_scatter.py\", line 168, in scatter\n",
      "    raise ImportError(\"'scatter' requires the 'torch-scatter' package\")\n",
      "ImportError: 'scatter' requires the 'torch-scatter' package\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 953, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1005, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 91, in __new__\n",
      "    elif 'noinherit' in styledefs and token is not Token:\n",
      "  File \"/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/pygments/style.py\", line 58, in colorformat\n",
      "    class StyleMeta(type):\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "import math\n",
    "import random\n",
    "from torch_geometric.data import DataLoader \n",
    "from bio_embeddings.embed import ProtTransBertBFDEmbedder, SeqVecEmbedder\n",
    "from torch_geometric.data import Dataset, download_url, Data,  Batch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "\n",
    "import biographs as bg\n",
    "from Bio import SeqIO\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, ids):\n",
    "\n",
    "        super().__init__()\n",
    "        self.ids = ids\n",
    "        \n",
    "    def len(self):\n",
    "        ## Can iter through len and load through batches\n",
    "        return len(self.ids)\n",
    "\n",
    "    def get(self, idx):\n",
    "        fname = self.ids[idx]\n",
    "        return torch.load(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "## masking some of the nodes in the training sites\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_files = ['./d/processed/data_1.pt', './d/processed/data_2.pt'] #file_names[0:2]\n",
    "#test_files = #file_names[2:4]\n",
    "\n",
    "train_dataset = ProteinDataset(ids=train_files)\n",
    "# test_dataset = ProteinDataset(ids=test_files)\n",
    "\n",
    "# train_dataset.process()\n",
    "# test_dataset.process()\n",
    "#data = data_class.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nguyjust/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset as Dataset_n\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "# print(math.floor(0.8 * size))\n",
    "# Make iterables using dataloader class\n",
    "#trainset, validset = torch.utils.data.random_split(\n",
    "#    data_class, [2, 2])\n",
    "# print(trainset[0])\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size = 2,  num_workers=0)\n",
    "#testloader = DataLoader(dataset=test_dataset, batch_size = 2, num_workers=0)\n",
    "# print(\"Length\")\n",
    "# print(len(trainloader))\n",
    "# print(len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = GCN(in_channels = 20, hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([184])) that is different to the input size (torch.Size([184, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m/Users/nguyjust/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/ubsite/scripts/proteins_to_graphs.py:28\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m#def test():\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#    model.eval()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#    out = model(trainloader.x, trainloader.edge_index)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#    return test_correct \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m30\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     29\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m/Users/nguyjust/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/ubsite/scripts/proteins_to_graphs.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[39m=\u001b[39m crit(out, data\u001b[39m.\u001b[39;49my)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/functional.py:3113\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3111\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3112\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3116\u001b[0m     )\n\u001b[1;32m   3118\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3119\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([184])) that is different to the input size (torch.Size([184, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([184])) that is different to the input size (torch.Size([184, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m/Users/nguyjust/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/ubsite/scripts/proteins_to_graphs.py:28\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m#def test():\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#    model.eval()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#    out = model(trainloader.x, trainloader.edge_index)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#    return test_correct \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m30\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     29\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m/Users/nguyjust/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/ubsite/scripts/proteins_to_graphs.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[39m=\u001b[39m crit(out, data\u001b[39m.\u001b[39;49my)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/functional.py:3113\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3111\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3112\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> 3113\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3116\u001b[0m     )\n\u001b[1;32m   3118\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3119\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([184])) that is different to the input size (torch.Size([184, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m/Users/nguyjust/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/ubsite/scripts/proteins_to_graphs.py:28\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m#def test():\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#    model.eval()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m#    out = model(trainloader.x, trainloader.edge_index)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m#    return test_correct \u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m30\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     29\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m/Users/nguyjust/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/ubsite/scripts/proteins_to_graphs.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      8\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index)\n\u001b[0;32m----> 9\u001b[0m loss \u001b[39m=\u001b[39m crit(out\u001b[39m.\u001b[39;49msqueeze(), data\u001b[39m.\u001b[39;49my)\n\u001b[1;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/modules/loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 618\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/py9/lib/python3.9/site-packages/torch/nn/functional.py:3122\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n\u001b[1;32m   3120\u001b[0m     weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39mexpand(new_size)\n\u001b[0;32m-> 3122\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight, reduction_enum)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[39m.\u001b[39my\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "model = GCN(in_channels = 20, hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.6847\n",
      "Epoch: 002, Loss: 0.6619\n",
      "Epoch: 003, Loss: 0.6310\n",
      "Epoch: 004, Loss: 0.6092\n",
      "Epoch: 005, Loss: 0.5783\n",
      "Epoch: 006, Loss: 0.5484\n",
      "Epoch: 007, Loss: 0.5128\n",
      "Epoch: 008, Loss: 0.4884\n",
      "Epoch: 009, Loss: 0.4509\n",
      "Epoch: 010, Loss: 0.4382\n",
      "Epoch: 011, Loss: 0.4030\n",
      "Epoch: 012, Loss: 0.3748\n",
      "Epoch: 013, Loss: 0.3577\n",
      "Epoch: 014, Loss: 0.3323\n",
      "Epoch: 015, Loss: 0.3039\n",
      "Epoch: 016, Loss: 0.2761\n",
      "Epoch: 017, Loss: 0.2393\n",
      "Epoch: 018, Loss: 0.2317\n",
      "Epoch: 019, Loss: 0.2230\n",
      "Epoch: 020, Loss: 0.1933\n",
      "Epoch: 021, Loss: 0.1897\n",
      "Epoch: 022, Loss: 0.1702\n",
      "Epoch: 023, Loss: 0.1571\n",
      "Epoch: 024, Loss: 0.1388\n",
      "Epoch: 025, Loss: 0.1258\n",
      "Epoch: 026, Loss: 0.1258\n",
      "Epoch: 027, Loss: 0.1126\n",
      "Epoch: 028, Loss: 0.1126\n",
      "Epoch: 029, Loss: 0.1102\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 30):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1229],\n",
       "        [0.0826],\n",
       "        [0.0554],\n",
       "        [0.0605],\n",
       "        [0.0495],\n",
       "        [0.0503],\n",
       "        [0.0348],\n",
       "        [0.0496],\n",
       "        [0.0231],\n",
       "        [0.0583],\n",
       "        [0.0418],\n",
       "        [0.0430],\n",
       "        [0.0562],\n",
       "        [0.0617],\n",
       "        [0.0803],\n",
       "        [0.0559],\n",
       "        [0.0488],\n",
       "        [0.0500],\n",
       "        [0.0427],\n",
       "        [0.0319],\n",
       "        [0.0302],\n",
       "        [0.0407],\n",
       "        [0.0331],\n",
       "        [0.0378],\n",
       "        [0.0466],\n",
       "        [0.0515],\n",
       "        [0.0437],\n",
       "        [0.0627],\n",
       "        [0.0824],\n",
       "        [0.0567],\n",
       "        [0.0974],\n",
       "        [0.0818],\n",
       "        [0.0860],\n",
       "        [0.1051],\n",
       "        [0.0813],\n",
       "        [0.0878],\n",
       "        [0.0588],\n",
       "        [0.0304],\n",
       "        [0.0667],\n",
       "        [0.0482],\n",
       "        [0.0491],\n",
       "        [0.0666],\n",
       "        [0.0854],\n",
       "        [0.0776],\n",
       "        [0.0884],\n",
       "        [0.0730],\n",
       "        [0.1343],\n",
       "        [0.1106],\n",
       "        [0.0536],\n",
       "        [0.0591],\n",
       "        [0.0930],\n",
       "        [0.0621],\n",
       "        [0.0531],\n",
       "        [0.0413],\n",
       "        [0.0992],\n",
       "        [0.0506],\n",
       "        [0.0267],\n",
       "        [0.0305],\n",
       "        [0.0519],\n",
       "        [0.0394],\n",
       "        [0.0258],\n",
       "        [0.0409],\n",
       "        [0.0357],\n",
       "        [0.0590],\n",
       "        [0.0794],\n",
       "        [0.0705],\n",
       "        [0.0619],\n",
       "        [0.0538],\n",
       "        [0.0626],\n",
       "        [0.0767],\n",
       "        [0.0447],\n",
       "        [0.0549],\n",
       "        [0.0765],\n",
       "        [0.1467],\n",
       "        [0.0633],\n",
       "        [0.1803],\n",
       "        [0.1686],\n",
       "        [0.0857],\n",
       "        [0.1055],\n",
       "        [0.0937],\n",
       "        [0.0908],\n",
       "        [0.0646],\n",
       "        [0.0555],\n",
       "        [0.0612],\n",
       "        [0.0688],\n",
       "        [0.0501],\n",
       "        [0.0839],\n",
       "        [0.0719],\n",
       "        [0.0704],\n",
       "        [0.0480],\n",
       "        [0.0974],\n",
       "        [0.0883],\n",
       "        [0.0800],\n",
       "        [0.0647],\n",
       "        [0.0442],\n",
       "        [0.0687],\n",
       "        [0.0349],\n",
       "        [0.0442],\n",
       "        [0.0282],\n",
       "        [0.0516],\n",
       "        [0.0213],\n",
       "        [0.0578],\n",
       "        [0.0350],\n",
       "        [0.0566],\n",
       "        [0.0696],\n",
       "        [0.0545],\n",
       "        [0.0600],\n",
       "        [0.0442],\n",
       "        [0.0301],\n",
       "        [0.0557],\n",
       "        [0.0433],\n",
       "        [0.0263],\n",
       "        [0.0450],\n",
       "        [0.0613],\n",
       "        [0.0350],\n",
       "        [0.0419],\n",
       "        [0.0683],\n",
       "        [0.0566],\n",
       "        [0.0319],\n",
       "        [0.0700],\n",
       "        [0.0917],\n",
       "        [0.0731],\n",
       "        [0.0826],\n",
       "        [0.0618],\n",
       "        [0.0908],\n",
       "        [0.0779],\n",
       "        [0.0634],\n",
       "        [0.0566],\n",
       "        [0.0453],\n",
       "        [0.0311],\n",
       "        [0.0708],\n",
       "        [0.0420],\n",
       "        [0.0367],\n",
       "        [0.0440],\n",
       "        [0.0610],\n",
       "        [0.0373],\n",
       "        [0.0647],\n",
       "        [0.0599],\n",
       "        [0.1218],\n",
       "        [0.0819],\n",
       "        [0.0352],\n",
       "        [0.0582],\n",
       "        [0.0855],\n",
       "        [0.0460],\n",
       "        [0.0443],\n",
       "        [0.0486],\n",
       "        [0.0865],\n",
       "        [0.0636],\n",
       "        [0.0360],\n",
       "        [0.0699],\n",
       "        [0.1098],\n",
       "        [0.0751],\n",
       "        [0.0543],\n",
       "        [0.0489],\n",
       "        [0.0416],\n",
       "        [0.0541],\n",
       "        [0.0703],\n",
       "        [0.0571],\n",
       "        [0.0524],\n",
       "        [0.0634],\n",
       "        [0.0605],\n",
       "        [0.0977],\n",
       "        [0.0606],\n",
       "        [0.0469],\n",
       "        [0.0771],\n",
       "        [0.1054],\n",
       "        [0.0547],\n",
       "        [0.0868],\n",
       "        [0.1165],\n",
       "        [0.0766],\n",
       "        [0.0988],\n",
       "        [0.1139],\n",
       "        [0.1228],\n",
       "        [0.0858],\n",
       "        [0.0749],\n",
       "        [0.0870],\n",
       "        [0.1009],\n",
       "        [0.0812],\n",
       "        [0.1082],\n",
       "        [0.1203],\n",
       "        [0.0927],\n",
       "        [0.0624],\n",
       "        [0.0882],\n",
       "        [0.0901]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1628, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data.x, data.edge_index).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.0984\n",
      "Epoch: 002, Loss: 0.0934\n",
      "Epoch: 003, Loss: 0.0825\n",
      "Epoch: 004, Loss: 0.0812\n",
      "Epoch: 005, Loss: 0.0778\n",
      "Epoch: 006, Loss: 0.0813\n",
      "Epoch: 007, Loss: 0.0739\n",
      "Epoch: 008, Loss: 0.0752\n",
      "Epoch: 009, Loss: 0.0725\n",
      "Epoch: 010, Loss: 0.0713\n",
      "Epoch: 011, Loss: 0.0747\n",
      "Epoch: 012, Loss: 0.0637\n",
      "Epoch: 013, Loss: 0.0669\n",
      "Epoch: 014, Loss: 0.0724\n",
      "Epoch: 015, Loss: 0.0652\n",
      "Epoch: 016, Loss: 0.0607\n",
      "Epoch: 017, Loss: 0.0629\n",
      "Epoch: 018, Loss: 0.0667\n",
      "Epoch: 019, Loss: 0.0604\n",
      "Epoch: 020, Loss: 0.0615\n",
      "Epoch: 021, Loss: 0.0650\n",
      "Epoch: 022, Loss: 0.0628\n",
      "Epoch: 023, Loss: 0.0599\n",
      "Epoch: 024, Loss: 0.0591\n",
      "Epoch: 025, Loss: 0.0651\n",
      "Epoch: 026, Loss: 0.0606\n",
      "Epoch: 027, Loss: 0.0659\n",
      "Epoch: 028, Loss: 0.0568\n",
      "Epoch: 029, Loss: 0.0573\n",
      "Epoch: 030, Loss: 0.0595\n",
      "Epoch: 031, Loss: 0.0643\n",
      "Epoch: 032, Loss: 0.0584\n",
      "Epoch: 033, Loss: 0.0643\n",
      "Epoch: 034, Loss: 0.0567\n",
      "Epoch: 035, Loss: 0.0582\n",
      "Epoch: 036, Loss: 0.0653\n",
      "Epoch: 037, Loss: 0.0688\n",
      "Epoch: 038, Loss: 0.0577\n",
      "Epoch: 039, Loss: 0.0614\n",
      "Epoch: 040, Loss: 0.0639\n",
      "Epoch: 041, Loss: 0.0587\n",
      "Epoch: 042, Loss: 0.0630\n",
      "Epoch: 043, Loss: 0.0594\n",
      "Epoch: 044, Loss: 0.0569\n",
      "Epoch: 045, Loss: 0.0648\n",
      "Epoch: 046, Loss: 0.0628\n",
      "Epoch: 047, Loss: 0.0557\n",
      "Epoch: 048, Loss: 0.0591\n",
      "Epoch: 049, Loss: 0.0568\n",
      "Epoch: 050, Loss: 0.0615\n",
      "Epoch: 051, Loss: 0.0590\n",
      "Epoch: 052, Loss: 0.0583\n",
      "Epoch: 053, Loss: 0.0682\n",
      "Epoch: 054, Loss: 0.0607\n",
      "Epoch: 055, Loss: 0.0568\n",
      "Epoch: 056, Loss: 0.0545\n",
      "Epoch: 057, Loss: 0.0682\n",
      "Epoch: 058, Loss: 0.0633\n",
      "Epoch: 059, Loss: 0.0587\n",
      "Epoch: 060, Loss: 0.0603\n",
      "Epoch: 061, Loss: 0.0623\n",
      "Epoch: 062, Loss: 0.0615\n",
      "Epoch: 063, Loss: 0.0599\n",
      "Epoch: 064, Loss: 0.0600\n",
      "Epoch: 065, Loss: 0.0610\n",
      "Epoch: 066, Loss: 0.0564\n",
      "Epoch: 067, Loss: 0.0592\n",
      "Epoch: 068, Loss: 0.0572\n",
      "Epoch: 069, Loss: 0.0583\n",
      "Epoch: 070, Loss: 0.0599\n",
      "Epoch: 071, Loss: 0.0570\n",
      "Epoch: 072, Loss: 0.0576\n",
      "Epoch: 073, Loss: 0.0539\n",
      "Epoch: 074, Loss: 0.0619\n",
      "Epoch: 075, Loss: 0.0570\n",
      "Epoch: 076, Loss: 0.0629\n",
      "Epoch: 077, Loss: 0.0615\n",
      "Epoch: 078, Loss: 0.0629\n",
      "Epoch: 079, Loss: 0.0620\n",
      "Epoch: 080, Loss: 0.0530\n",
      "Epoch: 081, Loss: 0.0569\n",
      "Epoch: 082, Loss: 0.0558\n",
      "Epoch: 083, Loss: 0.0609\n",
      "Epoch: 084, Loss: 0.0573\n",
      "Epoch: 085, Loss: 0.0631\n",
      "Epoch: 086, Loss: 0.0510\n",
      "Epoch: 087, Loss: 0.0597\n",
      "Epoch: 088, Loss: 0.0612\n",
      "Epoch: 089, Loss: 0.0593\n",
      "Epoch: 090, Loss: 0.0604\n",
      "Epoch: 091, Loss: 0.0565\n",
      "Epoch: 092, Loss: 0.0534\n",
      "Epoch: 093, Loss: 0.0576\n",
      "Epoch: 094, Loss: 0.0534\n",
      "Epoch: 095, Loss: 0.0566\n",
      "Epoch: 096, Loss: 0.0558\n",
      "Epoch: 097, Loss: 0.0595\n",
      "Epoch: 098, Loss: 0.0576\n",
      "Epoch: 099, Loss: 0.0560\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = crit(out.squeeze(), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0447, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model(data.x, data.edge_index).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint() received an invalid combination of arguments - got (int, int, size=int), but expected one of:\n * (int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int high, tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int low, int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int low, int high, tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/nguyjust/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/ubsite/scripts/proteins_to_graphs.py:34\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m#def test():\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[39m#    model.eval()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m#    out = model(trainloader.x, trainloader.edge_index)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m#    return test_correct \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m):\n\u001b[0;32m---> 34\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[1;32m     35\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m/Users/nguyjust/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/ubsite/scripts/proteins_to_graphs.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m pos_idx \u001b[39m=\u001b[39m (data\u001b[39m.\u001b[39my \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mnonzero(as_tuple\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m neg_idx \u001b[39m=\u001b[39m (data\u001b[39m.\u001b[39my \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnonzero(as_tuple\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m neg_idx \u001b[39m=\u001b[39m neg_idx[torch\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39mlen\u001b[39;49m(neg_idx), size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)]\n\u001b[1;32m     14\u001b[0m idx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((pos_idx, neg_idx), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m crit(out\u001b[39m.\u001b[39msqueeze()[idx], data\u001b[39m.\u001b[39my[idx])\n",
      "\u001b[0;31mTypeError\u001b[0m: randint() received an invalid combination of arguments - got (int, int, size=int), but expected one of:\n * (int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int high, tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int low, int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (int low, int high, tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pos_idx = (data.y == 1).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = (data.y == 0).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = neg_idx[torch.randint(0, len(neg_idx), size=10)]\n",
    "        idx = torch.cat((pos_idx, neg_idx), dim=-1)\n",
    "        loss = crit(out.squeeze()[idx], data.y[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7899\n",
      "Epoch: 002, Loss: 0.6791\n",
      "Epoch: 003, Loss: 0.6690\n",
      "Epoch: 004, Loss: 0.6525\n",
      "Epoch: 005, Loss: 0.6449\n",
      "Epoch: 006, Loss: 0.6410\n",
      "Epoch: 007, Loss: 0.5853\n",
      "Epoch: 008, Loss: 0.5589\n",
      "Epoch: 009, Loss: 0.5651\n",
      "Epoch: 010, Loss: 0.5275\n",
      "Epoch: 011, Loss: 0.5164\n",
      "Epoch: 012, Loss: 0.4913\n",
      "Epoch: 013, Loss: 0.4141\n",
      "Epoch: 014, Loss: 0.4054\n",
      "Epoch: 015, Loss: 0.4097\n",
      "Epoch: 016, Loss: 0.4278\n",
      "Epoch: 017, Loss: 0.3987\n",
      "Epoch: 018, Loss: 0.3477\n",
      "Epoch: 019, Loss: 0.3515\n",
      "Epoch: 020, Loss: 0.3721\n",
      "Epoch: 021, Loss: 0.3600\n",
      "Epoch: 022, Loss: 0.3817\n",
      "Epoch: 023, Loss: 0.4124\n",
      "Epoch: 024, Loss: 0.3734\n",
      "Epoch: 025, Loss: 0.3344\n",
      "Epoch: 026, Loss: 0.3589\n",
      "Epoch: 027, Loss: 0.3678\n",
      "Epoch: 028, Loss: 0.3527\n",
      "Epoch: 029, Loss: 0.3904\n",
      "Epoch: 030, Loss: 0.3864\n",
      "Epoch: 031, Loss: 0.3716\n",
      "Epoch: 032, Loss: 0.4155\n",
      "Epoch: 033, Loss: 0.3572\n",
      "Epoch: 034, Loss: 0.3628\n",
      "Epoch: 035, Loss: 0.3764\n",
      "Epoch: 036, Loss: 0.3331\n",
      "Epoch: 037, Loss: 0.3360\n",
      "Epoch: 038, Loss: 0.3424\n",
      "Epoch: 039, Loss: 0.3235\n",
      "Epoch: 040, Loss: 0.3134\n",
      "Epoch: 041, Loss: 0.3078\n",
      "Epoch: 042, Loss: 0.3509\n",
      "Epoch: 043, Loss: 0.3740\n",
      "Epoch: 044, Loss: 0.2685\n",
      "Epoch: 045, Loss: 0.3767\n",
      "Epoch: 046, Loss: 0.3149\n",
      "Epoch: 047, Loss: 0.3383\n",
      "Epoch: 048, Loss: 0.2935\n",
      "Epoch: 049, Loss: 0.2993\n",
      "Epoch: 050, Loss: 0.3172\n",
      "Epoch: 051, Loss: 0.2847\n",
      "Epoch: 052, Loss: 0.3610\n",
      "Epoch: 053, Loss: 0.3280\n",
      "Epoch: 054, Loss: 0.3394\n",
      "Epoch: 055, Loss: 0.3415\n",
      "Epoch: 056, Loss: 0.3110\n",
      "Epoch: 057, Loss: 0.3159\n",
      "Epoch: 058, Loss: 0.2916\n",
      "Epoch: 059, Loss: 0.2784\n",
      "Epoch: 060, Loss: 0.2808\n",
      "Epoch: 061, Loss: 0.3424\n",
      "Epoch: 062, Loss: 0.3164\n",
      "Epoch: 063, Loss: 0.2829\n",
      "Epoch: 064, Loss: 0.3293\n",
      "Epoch: 065, Loss: 0.2657\n",
      "Epoch: 066, Loss: 0.3177\n",
      "Epoch: 067, Loss: 0.3040\n",
      "Epoch: 068, Loss: 0.2710\n",
      "Epoch: 069, Loss: 0.3344\n",
      "Epoch: 070, Loss: 0.3401\n",
      "Epoch: 071, Loss: 0.3104\n",
      "Epoch: 072, Loss: 0.2829\n",
      "Epoch: 073, Loss: 0.2951\n",
      "Epoch: 074, Loss: 0.2881\n",
      "Epoch: 075, Loss: 0.3522\n",
      "Epoch: 076, Loss: 0.2607\n",
      "Epoch: 077, Loss: 0.2640\n",
      "Epoch: 078, Loss: 0.2575\n",
      "Epoch: 079, Loss: 0.2624\n",
      "Epoch: 080, Loss: 0.2935\n",
      "Epoch: 081, Loss: 0.2908\n",
      "Epoch: 082, Loss: 0.2405\n",
      "Epoch: 083, Loss: 0.2325\n",
      "Epoch: 084, Loss: 0.2314\n",
      "Epoch: 085, Loss: 0.3023\n",
      "Epoch: 086, Loss: 0.2892\n",
      "Epoch: 087, Loss: 0.2435\n",
      "Epoch: 088, Loss: 0.3345\n",
      "Epoch: 089, Loss: 0.3008\n",
      "Epoch: 090, Loss: 0.2481\n",
      "Epoch: 091, Loss: 0.3743\n",
      "Epoch: 092, Loss: 0.2861\n",
      "Epoch: 093, Loss: 0.2667\n",
      "Epoch: 094, Loss: 0.3345\n",
      "Epoch: 095, Loss: 0.3010\n",
      "Epoch: 096, Loss: 0.3008\n",
      "Epoch: 097, Loss: 0.2929\n",
      "Epoch: 098, Loss: 0.2552\n",
      "Epoch: 099, Loss: 0.2246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pos_idx = (data.y == 1).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = (data.y == 0).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = neg_idx[torch.randint(0, len(neg_idx), size=(10,))]\n",
    "        idx = torch.cat((pos_idx, neg_idx), dim=-1)\n",
    "        loss = crit(out.squeeze()[idx], data.y[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4729, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model(data.x, data.edge_index).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7554\n",
      "Epoch: 002, Loss: 0.7185\n",
      "Epoch: 003, Loss: 0.6658\n",
      "Epoch: 004, Loss: 0.7289\n",
      "Epoch: 005, Loss: 0.7822\n",
      "Epoch: 006, Loss: 0.6432\n",
      "Epoch: 007, Loss: 0.7016\n",
      "Epoch: 008, Loss: 0.5442\n",
      "Epoch: 009, Loss: 0.6866\n",
      "Epoch: 010, Loss: 0.5204\n",
      "Epoch: 011, Loss: 0.5414\n",
      "Epoch: 012, Loss: 0.5296\n",
      "Epoch: 013, Loss: 0.4783\n",
      "Epoch: 014, Loss: 0.6756\n",
      "Epoch: 015, Loss: 0.4889\n",
      "Epoch: 016, Loss: 0.6406\n",
      "Epoch: 017, Loss: 0.4898\n",
      "Epoch: 018, Loss: 0.5882\n",
      "Epoch: 019, Loss: 0.5964\n",
      "Epoch: 020, Loss: 0.4541\n",
      "Epoch: 021, Loss: 0.4252\n",
      "Epoch: 022, Loss: 0.5611\n",
      "Epoch: 023, Loss: 0.4414\n",
      "Epoch: 024, Loss: 0.4380\n",
      "Epoch: 025, Loss: 0.4644\n",
      "Epoch: 026, Loss: 0.4319\n",
      "Epoch: 027, Loss: 0.5579\n",
      "Epoch: 028, Loss: 0.4751\n",
      "Epoch: 029, Loss: 0.4279\n",
      "Epoch: 030, Loss: 0.4141\n",
      "Epoch: 031, Loss: 0.3931\n",
      "Epoch: 032, Loss: 0.4367\n",
      "Epoch: 033, Loss: 0.4029\n",
      "Epoch: 034, Loss: 0.3700\n",
      "Epoch: 035, Loss: 0.4602\n",
      "Epoch: 036, Loss: 0.4139\n",
      "Epoch: 037, Loss: 0.3438\n",
      "Epoch: 038, Loss: 0.3620\n",
      "Epoch: 039, Loss: 0.3777\n",
      "Epoch: 040, Loss: 0.4011\n",
      "Epoch: 041, Loss: 0.3501\n",
      "Epoch: 042, Loss: 0.5097\n",
      "Epoch: 043, Loss: 0.5665\n",
      "Epoch: 044, Loss: 0.3451\n",
      "Epoch: 045, Loss: 0.5959\n",
      "Epoch: 046, Loss: 0.3613\n",
      "Epoch: 047, Loss: 0.3386\n",
      "Epoch: 048, Loss: 0.4082\n",
      "Epoch: 049, Loss: 0.3364\n",
      "Epoch: 050, Loss: 0.6576\n",
      "Epoch: 051, Loss: 0.3536\n",
      "Epoch: 052, Loss: 0.3200\n",
      "Epoch: 053, Loss: 0.3036\n",
      "Epoch: 054, Loss: 0.3266\n",
      "Epoch: 055, Loss: 0.5540\n",
      "Epoch: 056, Loss: 0.2952\n",
      "Epoch: 057, Loss: 0.2952\n",
      "Epoch: 058, Loss: 0.4273\n",
      "Epoch: 059, Loss: 0.3370\n",
      "Epoch: 060, Loss: 0.3640\n",
      "Epoch: 061, Loss: 0.4607\n",
      "Epoch: 062, Loss: 0.2948\n",
      "Epoch: 063, Loss: 0.2949\n",
      "Epoch: 064, Loss: 0.3658\n",
      "Epoch: 065, Loss: 0.2770\n",
      "Epoch: 066, Loss: 0.2860\n",
      "Epoch: 067, Loss: 0.3699\n",
      "Epoch: 068, Loss: 0.3334\n",
      "Epoch: 069, Loss: 0.3369\n",
      "Epoch: 070, Loss: 0.3581\n",
      "Epoch: 071, Loss: 0.2669\n",
      "Epoch: 072, Loss: 0.3456\n",
      "Epoch: 073, Loss: 0.5821\n",
      "Epoch: 074, Loss: 0.2461\n",
      "Epoch: 075, Loss: 0.2528\n",
      "Epoch: 076, Loss: 0.2593\n",
      "Epoch: 077, Loss: 0.2476\n",
      "Epoch: 078, Loss: 0.3077\n",
      "Epoch: 079, Loss: 0.4365\n",
      "Epoch: 080, Loss: 0.6498\n",
      "Epoch: 081, Loss: 0.2363\n",
      "Epoch: 082, Loss: 0.4589\n",
      "Epoch: 083, Loss: 0.2594\n",
      "Epoch: 084, Loss: 0.2241\n",
      "Epoch: 085, Loss: 0.2613\n",
      "Epoch: 086, Loss: 0.2152\n",
      "Epoch: 087, Loss: 0.2886\n",
      "Epoch: 088, Loss: 0.2159\n",
      "Epoch: 089, Loss: 0.3035\n",
      "Epoch: 090, Loss: 0.2264\n",
      "Epoch: 091, Loss: 0.2778\n",
      "Epoch: 092, Loss: 0.5949\n",
      "Epoch: 093, Loss: 0.6342\n",
      "Epoch: 094, Loss: 0.6613\n",
      "Epoch: 095, Loss: 0.2120\n",
      "Epoch: 096, Loss: 0.2454\n",
      "Epoch: 097, Loss: 0.2016\n",
      "Epoch: 098, Loss: 0.2456\n",
      "Epoch: 099, Loss: 0.2033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pos_idx = (data.y == 1).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = (data.y == 0).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = neg_idx[torch.randint(0, len(neg_idx), size=(1,))]\n",
    "        idx = torch.cat((pos_idx, neg_idx), dim=-1)\n",
    "        loss = crit(out.squeeze()[idx], data.y[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7556, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model(data.x, data.edge_index).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (data\u001b[39m.\u001b[39;49my \u001b[39m==\u001b[39;49m (model(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index) \u001b[39m>\u001b[39;49m \u001b[39m0.5\u001b[39;49m))\u001b[39m.\u001b[39;49mmean()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Bool"
     ]
    }
   ],
   "source": [
    "\n",
    "(data.y == (model(data.x, data.edge_index) > 0.5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8509)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(1.*(data.y == (model(data.x, data.edge_index) > 0.5))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "model = GCN(in_channels = 20, hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7009\n",
      "Epoch: 002, Loss: 0.6985\n",
      "Epoch: 003, Loss: 0.6745\n",
      "Epoch: 004, Loss: 0.6614\n",
      "Epoch: 005, Loss: 0.6610\n",
      "Epoch: 006, Loss: 0.6562\n",
      "Epoch: 007, Loss: 0.6536\n",
      "Epoch: 008, Loss: 0.6339\n",
      "Epoch: 009, Loss: 0.6467\n",
      "Epoch: 010, Loss: 0.6355\n",
      "Epoch: 011, Loss: 0.5954\n",
      "Epoch: 012, Loss: 0.6197\n",
      "Epoch: 013, Loss: 0.6335\n",
      "Epoch: 014, Loss: 0.6006\n",
      "Epoch: 015, Loss: 0.5888\n",
      "Epoch: 016, Loss: 0.6549\n",
      "Epoch: 017, Loss: 0.5530\n",
      "Epoch: 018, Loss: 0.5711\n",
      "Epoch: 019, Loss: 0.5872\n",
      "Epoch: 020, Loss: 0.5938\n",
      "Epoch: 021, Loss: 0.5420\n",
      "Epoch: 022, Loss: 0.5125\n",
      "Epoch: 023, Loss: 0.6516\n",
      "Epoch: 024, Loss: 0.5626\n",
      "Epoch: 025, Loss: 0.5583\n",
      "Epoch: 026, Loss: 0.5312\n",
      "Epoch: 027, Loss: 0.6478\n",
      "Epoch: 028, Loss: 0.5505\n",
      "Epoch: 029, Loss: 0.5123\n",
      "Epoch: 030, Loss: 0.5208\n",
      "Epoch: 031, Loss: 0.5391\n",
      "Epoch: 032, Loss: 0.5368\n",
      "Epoch: 033, Loss: 0.5449\n",
      "Epoch: 034, Loss: 0.4462\n",
      "Epoch: 035, Loss: 0.4867\n",
      "Epoch: 036, Loss: 0.5392\n",
      "Epoch: 037, Loss: 0.6629\n",
      "Epoch: 038, Loss: 0.4791\n",
      "Epoch: 039, Loss: 0.4647\n",
      "Epoch: 040, Loss: 0.4449\n",
      "Epoch: 041, Loss: 0.4800\n",
      "Epoch: 042, Loss: 0.4725\n",
      "Epoch: 043, Loss: 0.4756\n",
      "Epoch: 044, Loss: 0.4188\n",
      "Epoch: 045, Loss: 0.4345\n",
      "Epoch: 046, Loss: 0.4466\n",
      "Epoch: 047, Loss: 0.4178\n",
      "Epoch: 048, Loss: 0.3910\n",
      "Epoch: 049, Loss: 0.3587\n",
      "Epoch: 050, Loss: 0.3736\n",
      "Epoch: 051, Loss: 0.4313\n",
      "Epoch: 052, Loss: 0.3104\n",
      "Epoch: 053, Loss: 0.7360\n",
      "Epoch: 054, Loss: 0.3151\n",
      "Epoch: 055, Loss: 0.5842\n",
      "Epoch: 056, Loss: 0.3751\n",
      "Epoch: 057, Loss: 0.5719\n",
      "Epoch: 058, Loss: 0.6952\n",
      "Epoch: 059, Loss: 0.3328\n",
      "Epoch: 060, Loss: 0.3126\n",
      "Epoch: 061, Loss: 0.3267\n",
      "Epoch: 062, Loss: 0.3133\n",
      "Epoch: 063, Loss: 0.2775\n",
      "Epoch: 064, Loss: 0.2858\n",
      "Epoch: 065, Loss: 0.3564\n",
      "Epoch: 066, Loss: 0.5506\n",
      "Epoch: 067, Loss: 0.3322\n",
      "Epoch: 068, Loss: 0.2838\n",
      "Epoch: 069, Loss: 0.5060\n",
      "Epoch: 070, Loss: 0.3710\n",
      "Epoch: 071, Loss: 0.2047\n",
      "Epoch: 072, Loss: 0.2114\n",
      "Epoch: 073, Loss: 0.7980\n",
      "Epoch: 074, Loss: 0.1881\n",
      "Epoch: 075, Loss: 0.3445\n",
      "Epoch: 076, Loss: 0.2178\n",
      "Epoch: 077, Loss: 0.2079\n",
      "Epoch: 078, Loss: 0.8296\n",
      "Epoch: 079, Loss: 0.1868\n",
      "Epoch: 080, Loss: 0.5721\n",
      "Epoch: 081, Loss: 0.3815\n",
      "Epoch: 082, Loss: 0.1824\n",
      "Epoch: 083, Loss: 0.1830\n",
      "Epoch: 084, Loss: 0.2933\n",
      "Epoch: 085, Loss: 0.6322\n",
      "Epoch: 086, Loss: 0.1816\n",
      "Epoch: 087, Loss: 0.1862\n",
      "Epoch: 088, Loss: 0.2803\n",
      "Epoch: 089, Loss: 0.2210\n",
      "Epoch: 090, Loss: 0.2803\n",
      "Epoch: 091, Loss: 0.9745\n",
      "Epoch: 092, Loss: 0.1607\n",
      "Epoch: 093, Loss: 0.1542\n",
      "Epoch: 094, Loss: 0.1460\n",
      "Epoch: 095, Loss: 0.3235\n",
      "Epoch: 096, Loss: 0.1552\n",
      "Epoch: 097, Loss: 0.1455\n",
      "Epoch: 098, Loss: 0.1309\n",
      "Epoch: 099, Loss: 0.1298\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pos_idx = (data.y == 1).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = (data.y == 0).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = neg_idx[torch.randint(0, len(neg_idx), size=(1,))]\n",
    "        idx = torch.cat((pos_idx, neg_idx), dim=-1)\n",
    "        loss = crit(out.squeeze()[idx], data.y[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7871)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "(1.*(data.y == (model(data.x, data.edge_index) > 0.5))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        #x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "model = GCN(in_channels = 20, hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0)\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "crit = torch.nn.BCELoss() # target is between 0,1, yhat is unnormalized logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7009\n",
      "Epoch: 002, Loss: 0.7073\n",
      "Epoch: 003, Loss: 0.6952\n",
      "Epoch: 004, Loss: 0.6920\n",
      "Epoch: 005, Loss: 0.7009\n",
      "Epoch: 006, Loss: 0.7030\n",
      "Epoch: 007, Loss: 0.6925\n",
      "Epoch: 008, Loss: 0.6969\n",
      "Epoch: 009, Loss: 0.6907\n",
      "Epoch: 010, Loss: 0.6926\n",
      "Epoch: 011, Loss: 0.6928\n",
      "Epoch: 012, Loss: 0.6952\n",
      "Epoch: 013, Loss: 0.6872\n",
      "Epoch: 014, Loss: 0.6919\n",
      "Epoch: 015, Loss: 0.6864\n",
      "Epoch: 016, Loss: 0.6897\n",
      "Epoch: 017, Loss: 0.6856\n",
      "Epoch: 018, Loss: 0.6801\n",
      "Epoch: 019, Loss: 0.6867\n",
      "Epoch: 020, Loss: 0.6924\n",
      "Epoch: 021, Loss: 0.6784\n",
      "Epoch: 022, Loss: 0.6685\n",
      "Epoch: 023, Loss: 0.6841\n",
      "Epoch: 024, Loss: 0.6852\n",
      "Epoch: 025, Loss: 0.6807\n",
      "Epoch: 026, Loss: 0.6786\n",
      "Epoch: 027, Loss: 0.6807\n",
      "Epoch: 028, Loss: 0.6757\n",
      "Epoch: 029, Loss: 0.6737\n",
      "Epoch: 030, Loss: 0.6788\n",
      "Epoch: 031, Loss: 0.6772\n",
      "Epoch: 032, Loss: 0.6709\n",
      "Epoch: 033, Loss: 0.6740\n",
      "Epoch: 034, Loss: 0.6535\n",
      "Epoch: 035, Loss: 0.6724\n",
      "Epoch: 036, Loss: 0.6658\n",
      "Epoch: 037, Loss: 0.6683\n",
      "Epoch: 038, Loss: 0.6604\n",
      "Epoch: 039, Loss: 0.6714\n",
      "Epoch: 040, Loss: 0.6614\n",
      "Epoch: 041, Loss: 0.6706\n",
      "Epoch: 042, Loss: 0.6731\n",
      "Epoch: 043, Loss: 0.6586\n",
      "Epoch: 044, Loss: 0.6618\n",
      "Epoch: 045, Loss: 0.6600\n",
      "Epoch: 046, Loss: 0.6549\n",
      "Epoch: 047, Loss: 0.6560\n",
      "Epoch: 048, Loss: 0.6550\n",
      "Epoch: 049, Loss: 0.6451\n",
      "Epoch: 050, Loss: 0.6538\n",
      "Epoch: 051, Loss: 0.6554\n",
      "Epoch: 052, Loss: 0.6347\n",
      "Epoch: 053, Loss: 0.6683\n",
      "Epoch: 054, Loss: 0.6374\n",
      "Epoch: 055, Loss: 0.6593\n",
      "Epoch: 056, Loss: 0.6609\n",
      "Epoch: 057, Loss: 0.6588\n",
      "Epoch: 058, Loss: 0.6530\n",
      "Epoch: 059, Loss: 0.6472\n",
      "Epoch: 060, Loss: 0.6366\n",
      "Epoch: 061, Loss: 0.6401\n",
      "Epoch: 062, Loss: 0.6273\n",
      "Epoch: 063, Loss: 0.6215\n",
      "Epoch: 064, Loss: 0.6273\n",
      "Epoch: 065, Loss: 0.6359\n",
      "Epoch: 066, Loss: 0.6466\n",
      "Epoch: 067, Loss: 0.6448\n",
      "Epoch: 068, Loss: 0.6372\n",
      "Epoch: 069, Loss: 0.6421\n",
      "Epoch: 070, Loss: 0.6333\n",
      "Epoch: 071, Loss: 0.6129\n",
      "Epoch: 072, Loss: 0.6182\n",
      "Epoch: 073, Loss: 0.6556\n",
      "Epoch: 074, Loss: 0.6096\n",
      "Epoch: 075, Loss: 0.6330\n",
      "Epoch: 076, Loss: 0.6272\n",
      "Epoch: 077, Loss: 0.6201\n",
      "Epoch: 078, Loss: 0.6449\n",
      "Epoch: 079, Loss: 0.6276\n",
      "Epoch: 080, Loss: 0.6381\n",
      "Epoch: 081, Loss: 0.6244\n",
      "Epoch: 082, Loss: 0.5929\n",
      "Epoch: 083, Loss: 0.5918\n",
      "Epoch: 084, Loss: 0.6217\n",
      "Epoch: 085, Loss: 0.6429\n",
      "Epoch: 086, Loss: 0.6298\n",
      "Epoch: 087, Loss: 0.6065\n",
      "Epoch: 088, Loss: 0.6186\n",
      "Epoch: 089, Loss: 0.6292\n",
      "Epoch: 090, Loss: 0.6357\n",
      "Epoch: 091, Loss: 0.6568\n",
      "Epoch: 092, Loss: 0.6152\n",
      "Epoch: 093, Loss: 0.5890\n",
      "Epoch: 094, Loss: 0.5968\n",
      "Epoch: 095, Loss: 0.6123\n",
      "Epoch: 096, Loss: 0.6205\n",
      "Epoch: 097, Loss: 0.5788\n",
      "Epoch: 098, Loss: 0.5954\n",
      "Epoch: 099, Loss: 0.5814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pos_idx = (data.y == 1).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = (data.y == 0).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = neg_idx[torch.randint(0, len(neg_idx), size=(1,))]\n",
    "        idx = torch.cat((pos_idx, neg_idx), dim=-1)\n",
    "        loss = crit(out.squeeze()[idx], data.y[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0109)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "(1.*(data.y == (model(data.x, data.edge_index) > 0.5))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5344],\n",
       "        [0.5386],\n",
       "        [0.5510],\n",
       "        [0.5554],\n",
       "        [0.5786],\n",
       "        [0.5887],\n",
       "        [0.6054],\n",
       "        [0.6034],\n",
       "        [0.6224],\n",
       "        [0.6124],\n",
       "        [0.6373],\n",
       "        [0.6382],\n",
       "        [0.6338],\n",
       "        [0.6345],\n",
       "        [0.6257],\n",
       "        [0.6046],\n",
       "        [0.5965],\n",
       "        [0.5783],\n",
       "        [0.5827],\n",
       "        [0.6027],\n",
       "        [0.5905],\n",
       "        [0.5816],\n",
       "        [0.5903],\n",
       "        [0.5903],\n",
       "        [0.5788],\n",
       "        [0.5767],\n",
       "        [0.5830],\n",
       "        [0.5677],\n",
       "        [0.5521],\n",
       "        [0.5672],\n",
       "        [0.5632],\n",
       "        [0.5798],\n",
       "        [0.5733],\n",
       "        [0.5786],\n",
       "        [0.5801],\n",
       "        [0.5812],\n",
       "        [0.5870],\n",
       "        [0.5959],\n",
       "        [0.5708],\n",
       "        [0.5724],\n",
       "        [0.5553],\n",
       "        [0.5547],\n",
       "        [0.5430],\n",
       "        [0.5668],\n",
       "        [0.5661],\n",
       "        [0.5777],\n",
       "        [0.5735],\n",
       "        [0.5763],\n",
       "        [0.5930],\n",
       "        [0.5999],\n",
       "        [0.5897],\n",
       "        [0.5965],\n",
       "        [0.6069],\n",
       "        [0.6106],\n",
       "        [0.5876],\n",
       "        [0.5952],\n",
       "        [0.6203],\n",
       "        [0.6151],\n",
       "        [0.5883],\n",
       "        [0.6155],\n",
       "        [0.6395],\n",
       "        [0.6186],\n",
       "        [0.6149],\n",
       "        [0.5969],\n",
       "        [0.5925],\n",
       "        [0.5799],\n",
       "        [0.5618],\n",
       "        [0.5605],\n",
       "        [0.5469],\n",
       "        [0.5448],\n",
       "        [0.5671],\n",
       "        [0.5704],\n",
       "        [0.5603],\n",
       "        [0.5415],\n",
       "        [0.5532],\n",
       "        [0.5354],\n",
       "        [0.5331],\n",
       "        [0.5477],\n",
       "        [0.5473],\n",
       "        [0.5527],\n",
       "        [0.5565],\n",
       "        [0.5594],\n",
       "        [0.5602],\n",
       "        [0.5622],\n",
       "        [0.5656],\n",
       "        [0.5712],\n",
       "        [0.5680],\n",
       "        [0.5767],\n",
       "        [0.5791],\n",
       "        [0.5801],\n",
       "        [0.5730],\n",
       "        [0.5785],\n",
       "        [0.5344],\n",
       "        [0.5386],\n",
       "        [0.5510],\n",
       "        [0.5554],\n",
       "        [0.5786],\n",
       "        [0.5887],\n",
       "        [0.6054],\n",
       "        [0.6034],\n",
       "        [0.6224],\n",
       "        [0.6124],\n",
       "        [0.6373],\n",
       "        [0.6382],\n",
       "        [0.6338],\n",
       "        [0.6345],\n",
       "        [0.6257],\n",
       "        [0.6046],\n",
       "        [0.5965],\n",
       "        [0.5783],\n",
       "        [0.5827],\n",
       "        [0.6027],\n",
       "        [0.5905],\n",
       "        [0.5816],\n",
       "        [0.5903],\n",
       "        [0.5903],\n",
       "        [0.5788],\n",
       "        [0.5767],\n",
       "        [0.5830],\n",
       "        [0.5677],\n",
       "        [0.5521],\n",
       "        [0.5672],\n",
       "        [0.5632],\n",
       "        [0.5798],\n",
       "        [0.5733],\n",
       "        [0.5786],\n",
       "        [0.5801],\n",
       "        [0.5812],\n",
       "        [0.5870],\n",
       "        [0.5959],\n",
       "        [0.5708],\n",
       "        [0.5724],\n",
       "        [0.5553],\n",
       "        [0.5547],\n",
       "        [0.5430],\n",
       "        [0.5668],\n",
       "        [0.5661],\n",
       "        [0.5777],\n",
       "        [0.5735],\n",
       "        [0.5763],\n",
       "        [0.5930],\n",
       "        [0.5999],\n",
       "        [0.5897],\n",
       "        [0.5965],\n",
       "        [0.6069],\n",
       "        [0.6106],\n",
       "        [0.5876],\n",
       "        [0.5952],\n",
       "        [0.6203],\n",
       "        [0.6151],\n",
       "        [0.5883],\n",
       "        [0.6155],\n",
       "        [0.6395],\n",
       "        [0.6186],\n",
       "        [0.6149],\n",
       "        [0.5969],\n",
       "        [0.5925],\n",
       "        [0.5799],\n",
       "        [0.5618],\n",
       "        [0.5605],\n",
       "        [0.5469],\n",
       "        [0.5448],\n",
       "        [0.5671],\n",
       "        [0.5704],\n",
       "        [0.5603],\n",
       "        [0.5415],\n",
       "        [0.5532],\n",
       "        [0.5354],\n",
       "        [0.5331],\n",
       "        [0.5477],\n",
       "        [0.5473],\n",
       "        [0.5527],\n",
       "        [0.5565],\n",
       "        [0.5594],\n",
       "        [0.5602],\n",
       "        [0.5622],\n",
       "        [0.5656],\n",
       "        [0.5712],\n",
       "        [0.5680],\n",
       "        [0.5767],\n",
       "        [0.5791],\n",
       "        [0.5801],\n",
       "        [0.5730],\n",
       "        [0.5785]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7880\n",
      "Epoch: 002, Loss: 0.7892\n",
      "Epoch: 003, Loss: 0.8299\n",
      "Epoch: 004, Loss: 0.8146\n",
      "Epoch: 005, Loss: 0.7964\n",
      "Epoch: 006, Loss: 0.8216\n",
      "Epoch: 007, Loss: 0.7842\n",
      "Epoch: 008, Loss: 0.7921\n",
      "Epoch: 009, Loss: 0.7836\n",
      "Epoch: 010, Loss: 0.7619\n",
      "Epoch: 011, Loss: 0.7765\n",
      "Epoch: 012, Loss: 0.7948\n",
      "Epoch: 013, Loss: 0.7475\n",
      "Epoch: 014, Loss: 0.7773\n",
      "Epoch: 015, Loss: 0.7822\n",
      "Epoch: 016, Loss: 0.7391\n",
      "Epoch: 017, Loss: 0.7629\n",
      "Epoch: 018, Loss: 0.7569\n",
      "Epoch: 019, Loss: 0.7227\n",
      "Epoch: 020, Loss: 0.7451\n",
      "Epoch: 021, Loss: 0.7337\n",
      "Epoch: 022, Loss: 0.7341\n",
      "Epoch: 023, Loss: 0.7361\n",
      "Epoch: 024, Loss: 0.7092\n",
      "Epoch: 025, Loss: 0.7219\n",
      "Epoch: 026, Loss: 0.7372\n",
      "Epoch: 027, Loss: 0.7189\n",
      "Epoch: 028, Loss: 0.6954\n",
      "Epoch: 029, Loss: 0.7114\n",
      "Epoch: 030, Loss: 0.6814\n",
      "Epoch: 031, Loss: 0.6844\n",
      "Epoch: 032, Loss: 0.7056\n",
      "Epoch: 033, Loss: 0.7054\n",
      "Epoch: 034, Loss: 0.6728\n",
      "Epoch: 035, Loss: 0.6659\n",
      "Epoch: 036, Loss: 0.6640\n",
      "Epoch: 037, Loss: 0.6715\n",
      "Epoch: 038, Loss: 0.6585\n",
      "Epoch: 039, Loss: 0.6620\n",
      "Epoch: 040, Loss: 0.6678\n",
      "Epoch: 041, Loss: 0.6615\n",
      "Epoch: 042, Loss: 0.6707\n",
      "Epoch: 043, Loss: 0.6616\n",
      "Epoch: 044, Loss: 0.6361\n",
      "Epoch: 045, Loss: 0.6447\n",
      "Epoch: 046, Loss: 0.6461\n",
      "Epoch: 047, Loss: 0.6301\n",
      "Epoch: 048, Loss: 0.6300\n",
      "Epoch: 049, Loss: 0.6275\n",
      "Epoch: 050, Loss: 0.6113\n",
      "Epoch: 051, Loss: 0.6096\n",
      "Epoch: 052, Loss: 0.6224\n",
      "Epoch: 053, Loss: 0.6163\n",
      "Epoch: 054, Loss: 0.6184\n",
      "Epoch: 055, Loss: 0.6073\n",
      "Epoch: 056, Loss: 0.6081\n",
      "Epoch: 057, Loss: 0.6024\n",
      "Epoch: 058, Loss: 0.5821\n",
      "Epoch: 059, Loss: 0.5939\n",
      "Epoch: 060, Loss: 0.5855\n",
      "Epoch: 061, Loss: 0.5901\n",
      "Epoch: 062, Loss: 0.5935\n",
      "Epoch: 063, Loss: 0.5755\n",
      "Epoch: 064, Loss: 0.5891\n",
      "Epoch: 065, Loss: 0.5714\n",
      "Epoch: 066, Loss: 0.5829\n",
      "Epoch: 067, Loss: 0.5675\n",
      "Epoch: 068, Loss: 0.5617\n",
      "Epoch: 069, Loss: 0.5695\n",
      "Epoch: 070, Loss: 0.5538\n",
      "Epoch: 071, Loss: 0.5589\n",
      "Epoch: 072, Loss: 0.5435\n",
      "Epoch: 073, Loss: 0.5473\n",
      "Epoch: 074, Loss: 0.5675\n",
      "Epoch: 075, Loss: 0.5487\n",
      "Epoch: 076, Loss: 0.5521\n",
      "Epoch: 077, Loss: 0.5496\n",
      "Epoch: 078, Loss: 0.5268\n",
      "Epoch: 079, Loss: 0.5286\n",
      "Epoch: 080, Loss: 0.5214\n",
      "Epoch: 081, Loss: 0.5232\n",
      "Epoch: 082, Loss: 0.5293\n",
      "Epoch: 083, Loss: 0.5268\n",
      "Epoch: 084, Loss: 0.5242\n",
      "Epoch: 085, Loss: 0.5391\n",
      "Epoch: 086, Loss: 0.5413\n",
      "Epoch: 087, Loss: 0.5114\n",
      "Epoch: 088, Loss: 0.5124\n",
      "Epoch: 089, Loss: 0.5321\n",
      "Epoch: 090, Loss: 0.4960\n",
      "Epoch: 091, Loss: 0.5162\n",
      "Epoch: 092, Loss: 0.4934\n",
      "Epoch: 093, Loss: 0.4982\n",
      "Epoch: 094, Loss: 0.4883\n",
      "Epoch: 095, Loss: 0.5166\n",
      "Epoch: 096, Loss: 0.4898\n",
      "Epoch: 097, Loss: 0.4847\n",
      "Epoch: 098, Loss: 0.4875\n",
      "Epoch: 099, Loss: 0.4874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for batch_idx, data in enumerate(trainloader): \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pos_idx = (data.y == 1).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = (data.y == 0).nonzero(as_tuple=True)[0]\n",
    "        neg_idx = neg_idx[torch.randint(0, len(neg_idx), size=(10,))]\n",
    "        idx = torch.cat((pos_idx, neg_idx), dim=-1)\n",
    "        loss = crit(out.squeeze()[idx], data.y[idx])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item() # .detach \n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    model.eval()\n",
    "#    out = model(trainloader.x, trainloader.edge_index)\n",
    "#    pred = out.argmax(dim=1)\n",
    "#    test_correct = pred[testloader.y]\n",
    "#    #test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "#    return test_correct \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3438],\n",
       "        [0.3092],\n",
       "        [0.2694],\n",
       "        [0.2921],\n",
       "        [0.2680],\n",
       "        [0.3017],\n",
       "        [0.2923],\n",
       "        [0.3408],\n",
       "        [0.2846],\n",
       "        [0.3686],\n",
       "        [0.3434],\n",
       "        [0.3825],\n",
       "        [0.3914],\n",
       "        [0.3856],\n",
       "        [0.3821],\n",
       "        [0.3372],\n",
       "        [0.3104],\n",
       "        [0.3133],\n",
       "        [0.3069],\n",
       "        [0.2887],\n",
       "        [0.2890],\n",
       "        [0.3041],\n",
       "        [0.2913],\n",
       "        [0.2957],\n",
       "        [0.3235],\n",
       "        [0.3192],\n",
       "        [0.2843],\n",
       "        [0.3221],\n",
       "        [0.3318],\n",
       "        [0.3134],\n",
       "        [0.3322],\n",
       "        [0.3224],\n",
       "        [0.3286],\n",
       "        [0.3435],\n",
       "        [0.3326],\n",
       "        [0.3387],\n",
       "        [0.3054],\n",
       "        [0.2720],\n",
       "        [0.2988],\n",
       "        [0.2907],\n",
       "        [0.2887],\n",
       "        [0.2947],\n",
       "        [0.3078],\n",
       "        [0.2968],\n",
       "        [0.3014],\n",
       "        [0.2989],\n",
       "        [0.3417],\n",
       "        [0.3224],\n",
       "        [0.2823],\n",
       "        [0.3045],\n",
       "        [0.3328],\n",
       "        [0.3048],\n",
       "        [0.3043],\n",
       "        [0.3202],\n",
       "        [0.3479],\n",
       "        [0.3282],\n",
       "        [0.3228],\n",
       "        [0.3642],\n",
       "        [0.3802],\n",
       "        [0.3784],\n",
       "        [0.3577],\n",
       "        [0.3619],\n",
       "        [0.3312],\n",
       "        [0.3530],\n",
       "        [0.3280],\n",
       "        [0.3105],\n",
       "        [0.2930],\n",
       "        [0.3077],\n",
       "        [0.3071],\n",
       "        [0.3213],\n",
       "        [0.2945],\n",
       "        [0.3018],\n",
       "        [0.3179],\n",
       "        [0.3156],\n",
       "        [0.2806],\n",
       "        [0.3111],\n",
       "        [0.3215],\n",
       "        [0.2986],\n",
       "        [0.3265],\n",
       "        [0.3400],\n",
       "        [0.3462],\n",
       "        [0.3157],\n",
       "        [0.3096],\n",
       "        [0.3244],\n",
       "        [0.3389],\n",
       "        [0.3065],\n",
       "        [0.3361],\n",
       "        [0.3460],\n",
       "        [0.3403],\n",
       "        [0.3088],\n",
       "        [0.3384],\n",
       "        [0.3487],\n",
       "        [0.3438],\n",
       "        [0.3092],\n",
       "        [0.2694],\n",
       "        [0.2921],\n",
       "        [0.2680],\n",
       "        [0.3017],\n",
       "        [0.2923],\n",
       "        [0.3408],\n",
       "        [0.2846],\n",
       "        [0.3686],\n",
       "        [0.3434],\n",
       "        [0.3825],\n",
       "        [0.3914],\n",
       "        [0.3856],\n",
       "        [0.3821],\n",
       "        [0.3372],\n",
       "        [0.3104],\n",
       "        [0.3133],\n",
       "        [0.3069],\n",
       "        [0.2887],\n",
       "        [0.2890],\n",
       "        [0.3041],\n",
       "        [0.2913],\n",
       "        [0.2957],\n",
       "        [0.3235],\n",
       "        [0.3192],\n",
       "        [0.2843],\n",
       "        [0.3221],\n",
       "        [0.3318],\n",
       "        [0.3134],\n",
       "        [0.3322],\n",
       "        [0.3224],\n",
       "        [0.3286],\n",
       "        [0.3435],\n",
       "        [0.3326],\n",
       "        [0.3387],\n",
       "        [0.3054],\n",
       "        [0.2720],\n",
       "        [0.2988],\n",
       "        [0.2907],\n",
       "        [0.2887],\n",
       "        [0.2947],\n",
       "        [0.3078],\n",
       "        [0.2968],\n",
       "        [0.3014],\n",
       "        [0.2989],\n",
       "        [0.3417],\n",
       "        [0.3224],\n",
       "        [0.2823],\n",
       "        [0.3045],\n",
       "        [0.3328],\n",
       "        [0.3048],\n",
       "        [0.3043],\n",
       "        [0.3202],\n",
       "        [0.3479],\n",
       "        [0.3282],\n",
       "        [0.3228],\n",
       "        [0.3642],\n",
       "        [0.3802],\n",
       "        [0.3784],\n",
       "        [0.3577],\n",
       "        [0.3619],\n",
       "        [0.3312],\n",
       "        [0.3530],\n",
       "        [0.3280],\n",
       "        [0.3105],\n",
       "        [0.2930],\n",
       "        [0.3077],\n",
       "        [0.3071],\n",
       "        [0.3213],\n",
       "        [0.2945],\n",
       "        [0.3018],\n",
       "        [0.3179],\n",
       "        [0.3156],\n",
       "        [0.2806],\n",
       "        [0.3111],\n",
       "        [0.3215],\n",
       "        [0.2986],\n",
       "        [0.3265],\n",
       "        [0.3400],\n",
       "        [0.3462],\n",
       "        [0.3157],\n",
       "        [0.3096],\n",
       "        [0.3244],\n",
       "        [0.3389],\n",
       "        [0.3065],\n",
       "        [0.3361],\n",
       "        [0.3460],\n",
       "        [0.3403],\n",
       "        [0.3088],\n",
       "        [0.3384],\n",
       "        [0.3487]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9891)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "(1.*(data.y == (model(data.x, data.edge_index) > 0.5))).mean()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
